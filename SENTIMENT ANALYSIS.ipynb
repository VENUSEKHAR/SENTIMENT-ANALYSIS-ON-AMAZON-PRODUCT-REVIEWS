{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb10db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ef01de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "273896d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \n",
       "0       5  I feel so LUCKY to have found this used (phone...           1.0  \n",
       "1       4  nice phone, nice up grade from my pantach revu...           0.0  \n",
       "2       5                                       Very pleased           0.0  \n",
       "3       4  It works good but it goes slow sometimes but i...           0.0  \n",
       "4       4  Great phone to replace my lost phone. The only...           0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load csv file\n",
    "df = pd.read_csv('Amazon_Unlocked_Mobile.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b16519b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(413840, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0aebee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA EXPLORATION BLOCK\n",
    "\n",
    "## VISUALISATION OF RATINGS\n",
    "\n",
    "## PLOTTING NO OF REVIEWS FOR TOP 20 BRANDS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd96eabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134801</th>\n",
       "      <td>BLU Studio 5.0 C HD - Unlocked Cell Phones - R...</td>\n",
       "      <td>BLU</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>5</td>\n",
       "      <td>For the price I paid for this devices, its fan...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123493</th>\n",
       "      <td>Blu LIFE 8 Unlocked (Pink)</td>\n",
       "      <td>BLU</td>\n",
       "      <td>199.98</td>\n",
       "      <td>5</td>\n",
       "      <td>love love love it....good buy...recommend to a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335592</th>\n",
       "      <td>Samsung Galaxy S Duos II S7582 DUAL SIM Factor...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>299.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Good</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246353</th>\n",
       "      <td>Motorola Droid 2 A955 Verizon Phone 5MP Cam, W...</td>\n",
       "      <td>Motorola</td>\n",
       "      <td>82.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Not good. Returned first phone and they sent m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273324</th>\n",
       "      <td>Nokia Lumia 920 32GB Unlocked GSM 4G LTE Windo...</td>\n",
       "      <td>Nokia</td>\n",
       "      <td>149.35</td>\n",
       "      <td>4</td>\n",
       "      <td>Met expectations! I'm very satisfied!Even arri...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Product Name Brand Name    Price  \\\n",
       "134801  BLU Studio 5.0 C HD - Unlocked Cell Phones - R...        BLU  2000.00   \n",
       "123493                         Blu LIFE 8 Unlocked (Pink)        BLU   199.98   \n",
       "335592  Samsung Galaxy S Duos II S7582 DUAL SIM Factor...    Samsung   299.99   \n",
       "246353  Motorola Droid 2 A955 Verizon Phone 5MP Cam, W...   Motorola    82.00   \n",
       "273324  Nokia Lumia 920 32GB Unlocked GSM 4G LTE Windo...      Nokia   149.35   \n",
       "\n",
       "        Rating                                            Reviews  \\\n",
       "134801       5  For the price I paid for this devices, its fan...   \n",
       "123493       5  love love love it....good buy...recommend to a...   \n",
       "335592       4                                               Good   \n",
       "246353       1  Not good. Returned first phone and they sent m...   \n",
       "273324       4  Met expectations! I'm very satisfied!Even arri...   \n",
       "\n",
       "        Review Votes  Sentiment  \n",
       "134801           0.0          1  \n",
       "123493           0.0          1  \n",
       "335592           0.0          1  \n",
       "246353           0.0          0  \n",
       "273324           1.0          1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=0.1, random_state=0) #SINCE DATA IS LARGER...CONSIDERING SMALL PART OF DATA FOR TRIANING\n",
    "\n",
    "# Drop missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove any 'neutral' ratings equal to 3\n",
    "df = df[df['Rating'] != 3]\n",
    "\n",
    "# Encode 4s and 5s as 1 (positive sentiment) and 1s and 2s as 0 (negative sentiment)\n",
    "df['Sentiment'] = np.where(df['Rating'] > 3, 1, 0)\n",
    "df.head()\n",
    "\n",
    "#FINALLY TRYING MAKE REVIEWS INTO 2CLASSES...EITHER GOOD REVIEW(1) OR BAD REVIEW(0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d984eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30888, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1691caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#to check balance between positie and negative reviews\n",
    "\n",
    "df[\"Sentiment\"].plot.hist(bins=3)\n",
    "plt.title('Classes distribution in the train data')\n",
    "plt.xlim(-0.5, 2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb186d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b619ea3b",
   "metadata": {},
   "source": [
    "##### Review is good or bad decided based on rating ... here it is SENTIMENT classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f992c77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 27799 training examples and 3089 validation examples. \n",
      "\n",
      "Show a review in the training set : \n",
      " Very good\n"
     ]
    }
   ],
   "source": [
    "# Split data into training set and validation with same ratio of classes in data  \n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Reviews'], df['Sentiment'], test_size=0.1,stratify=df['Sentiment'], random_state=42)\n",
    "\n",
    "print('Load %d training examples and %d validation examples. \\n' %(X_train.shape[0],X_test.shape[0]))\n",
    "print('Show a review in the training set : \\n', X_train.iloc[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953f53fb",
   "metadata": {},
   "source": [
    "## TEXT PREPROCESSING\n",
    "\n",
    "The following text preprocessing are implemented to convert raw reviews to cleaned review, so that it will be easier for us to do feature extraction in the next step.\n",
    "* remove html tags using BeautifulSoup\n",
    "* remove non-character such as digits and symbols\n",
    "* convert to lower case\n",
    "* remove stop words such as \"the\" and \"and\" if needed\n",
    "* convert to root words by stemming if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca08ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d732d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTION TO REMOVE STOP WORDS AND DO STEMMING\n",
    "\n",
    "def cleanText(raw_text, remove_stopwords=False, stemming=False, split_text=False, \\):\n",
    "    \n",
    "    text = BeautifulSoup(raw_text, 'lxml').get_text()         # remove html\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)             # remove non-character\n",
    "    words = letters_only.lower().split()                      # convert to lower case \n",
    "    \n",
    "    if remove_stopwords: # remove stopword\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        \n",
    "    if stemming==True: # stemming\n",
    "        stemmer = SnowballStemmer('english') \n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "        \n",
    "    if split_text==True:  # split text\n",
    "        return (words)\n",
    "    \n",
    "    return( \" \".join(words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b71696e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "619bbd84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show a cleaned review in the training set : \n",
      " very good\n"
     ]
    }
   ],
   "source": [
    "# APPENDING THE CLEANED DATA INTO AN EMPTY LIST\n",
    "\n",
    "# Preprocess text data in training set and validation set\n",
    "X_train_cleaned = []\n",
    "X_test_cleaned = []\n",
    "\n",
    "for d in X_train:\n",
    "    X_train_cleaned.append(cleanText(d))\n",
    "print('Show a cleaned review in the training set : \\n',  X_train_cleaned[10])\n",
    "    \n",
    "for d in X_test:\n",
    "    X_test_cleaned.append(cleanText(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd667461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e47660a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTION FOR METRICS\n",
    "\n",
    "def modelEvaluation(predictions):\n",
    "    print (\"\\nAccuracy on validation set: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
    "    print(\"\\nAUC score : {:.4f}\".format(roc_auc_score(y_test, predictions)))\n",
    "    print(\"\\nClassification report : \\n\", metrics.classification_report(y_test, predictions))\n",
    "    print(\"\\nConfusion Matrix : \\n\", metrics.confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f8eb4",
   "metadata": {},
   "source": [
    "##### TFIDF VECTORIZER WITH LOGISTIC REGRESSION AS CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "077b7a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2032b064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features : 5973 \n",
      "\n",
      "Show some feature names : \n",
      " ['00', 'challenge', 'factory', 'letdown', 'primarily', 'states']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform the training data to a document-term matrix using TfidfVectorizer \n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=5) #minimum document frequency of 5\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "print(\"Number of features : %d \\n\" %len(tfidf.get_feature_names())) #1722\n",
    "print(\"Show some feature names : \\n\", tfidf.get_feature_names()[::1000])\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7075e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 features with smallest coefficients :\n",
      "['not' 'return' 'disappointed' 'horrible' 'poor' 'waste' 'slow' 'worst'\n",
      " 'stopped' 'terrible']\n",
      "\n",
      "Top 10 features with largest coefficients : \n",
      "['great' 'love' 'excellent' 'perfect' 'good' 'best' 'price' 'easy' 'far'\n",
      " 'awesome']\n"
     ]
    }
   ],
   "source": [
    "# Look at the top 10 features with smallest and the largest coefficients\n",
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "sorted_coef_index = lr.coef_[0].argsort()\n",
    "print('\\nTop 10 features with smallest coefficients :\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Top 10 features with largest coefficients : \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e452eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set: 0.9291\n",
      "\n",
      "AUC score : 0.8938\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86       798\n",
      "           1       0.94      0.97      0.95      2291\n",
      "\n",
      "    accuracy                           0.93      3089\n",
      "   macro avg       0.92      0.89      0.90      3089\n",
      "weighted avg       0.93      0.93      0.93      3089\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 655  143]\n",
      " [  76 2215]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the validaton set\n",
    "predictions = lr.predict(tfidf.transform(X_test_cleaned))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae03122b",
   "metadata": {},
   "source": [
    "###### WORD2VEC AND RANDOM FOREST CLASSIFIER  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "06db6f8d",
   "metadata": {},
   "source": [
    "Another common approach of word embedding is prediction based embedding, such as Word2Vec model. In gist, Word2Vec is a combination of two techniques: Continuous Bag of Words (CBoW) and skip-gram model. Both are shallow neural networks which learn weights for word vector representations.\n",
    "\n",
    "In this part, we will train Word2Vec model to create our own word vector representations using gensim library. Then we fit the feature vectors of the reviews to Random Forest Classifier. Here's the workflow of this part.\n",
    "\n",
    "* Step 1 : Parse review text to sentences (Word2Vec model takes a list of sentences as inputs)\n",
    "* Step 2 : Create volcabulary list using Word2Vec model\n",
    "* Step 3 : Transform each review into numerical representation by computing average feature vectors of words \n",
    "* Step 4 : Fit the average feature vectors to Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fe7e409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\venus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0beff41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0e66cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27771 parsed sentence in the training set\n",
      "\n",
      "Show a parsed sentence in the training set : \n",
      " ['well', 'it', 'never', 'charged', 'from', 'the', 'time', 'we', 'got', 'and', 'to', 'have', 'this', 'fixed', 'it', 's', 'going', 'to', 'cost', 'me', 'another', 'so', 'not', 'happy', 'with', 'the', 'purchase', 'at', 'all', 'this', 'is', 'a', 'review', 'on', 'the', 'galaxy', 'i', 'purchased']\n"
     ]
    }
   ],
   "source": [
    "# Split review text into parsed sentences uisng NLTK's punkt tokenizer\n",
    "# nltk.download()\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def parseSent(review, tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(cleanText(raw_sentence, remove_stopwords, split_text=True))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Parse each review in the training set into sentences\n",
    "sentences = []\n",
    "for review in X_train_cleaned:\n",
    "    sentences += parseSent(review, tokenizer)\n",
    "    \n",
    "print('%d parsed sentence in the training set\\n'  %len(sentences))\n",
    "print('Show a parsed sentence in the training set : \\n',  sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b7eff9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model ...\n",
      "\n",
      "Number of words in the vocabulary list : 4008 \n",
      "\n",
      "Show first 10 words in the vocalbulary list  vocabulary list: \n",
      " ['the', 'i', 'it', 'and', 'phone', 'a', 'to', 'is', 'this', 'for']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-6971b3131eeb>:12: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "# Fit parsed sentences to Word2Vec model \n",
    "\n",
    "num_features = 300  #embedding dimension                     \n",
    "min_word_count = 10                \n",
    "num_workers = 4       \n",
    "context = 10                                                                                          \n",
    "downsampling = 1e-3 \n",
    "\n",
    "print(\"Training Word2Vec model ...\\n\")\n",
    "w2v = Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count = min_word_count,\\\n",
    "                 window = context, sample = downsampling)\n",
    "w2v.init_sims(replace=True)\n",
    "w2v.save(\"w2v_300features_10minwordcounts_10context\") #save trained word2vec model\n",
    "\n",
    "print(\"Number of words in the vocabulary list : %d \\n\" %len(w2v.wv.index_to_key)) #4016 \n",
    "print(\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\", w2v.wv.index_to_key [0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78a50a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfrom the training data into feature vectors\n",
    "\n",
    "def makeFeatureVec(review, model, num_features):\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index_to_key ) #index2word is the volcabulary list of the Word2Vec model\n",
    "    isZeroVec = True\n",
    "    for word in review:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec +=  model.wv.get_index(word)\n",
    "            isZeroVec = False\n",
    "    if isZeroVec == False:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ab8d768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\venus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96a42f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\venus\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.amazon.com/dp/B00K15KRV6/ref=cm_cr_ryp_prd_ttl_sol_22\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\venus\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:417: MarkupResemblesLocatorWarning: \"http://www.amazon.com/gp/product/B013YDFH3Y?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\venus\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:417: MarkupResemblesLocatorWarning: \"http://www.amazon.com/gp/product/B0193D539M?redirect=true&ref_=cm_cr_ryp_prd_ttl_sol_0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set : 27799 feature vectors with 300 dimensions\n",
      "Validation set : 3089 feature vectors with 300 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Get feature vectors for training set\n",
    "X_train_cleaned = []\n",
    "for review in X_train:\n",
    "    X_train_cleaned.append(cleanText(review, remove_stopwords=True, split_text=True))\n",
    "trainVector = getAvgFeatureVecs(X_train_cleaned, w2v, num_features)\n",
    "print(\"Training set : %d feature vectors with %d dimensions\" %trainVector.shape)\n",
    "\n",
    "\n",
    "# Get feature vectors for validation set\n",
    "X_test_cleaned = []\n",
    "for review in X_test:\n",
    "    X_test_cleaned.append(cleanText(review, remove_stopwords=True, split_text=True))\n",
    "testVector = getAvgFeatureVecs(X_test_cleaned, w2v, num_features)\n",
    "print(\"Validation set : %d feature vectors with %d dimensions\" %testVector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50fa278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62ad4a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set: 0.7388\n",
      "\n",
      "AUC score : 0.6434\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.45      0.47       798\n",
      "           1       0.81      0.84      0.83      2291\n",
      "\n",
      "    accuracy                           0.74      3089\n",
      "   macro avg       0.65      0.64      0.65      3089\n",
      "weighted avg       0.73      0.74      0.73      3089\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 356  442]\n",
      " [ 365 1926]]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(trainVector, y_train)\n",
    "predictions = rf.predict(testVector)\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec5ed9b",
   "metadata": {},
   "source": [
    "#### USING TOKENISER AND LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d1f2ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c55fbc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (27799, 100)\n",
      "X_test shape: (3089, 100)\n",
      "y_train shape: (27799, 2)\n",
      "y_test shape: (3089, 2)\n"
     ]
    }
   ],
   "source": [
    "top_words = 20000 \n",
    "maxlen = 100 \n",
    "batch_size = 32\n",
    "nb_classes = 2\n",
    "nb_epoch = 10\n",
    "\n",
    "\n",
    "# Vectorize X_train and X_test to 2D tensor\n",
    "tokenizer = Tokenizer(num_words=top_words) #only consider top 20000 words in the corpse\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# tokenizer.word_index #access word-to-index dictionary of trained tokenizer\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_seq =pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test_seq = pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "# one-hot encoding of y_train and y_test\n",
    "y_train_seq = np_utils.to_categorical((y_train).astype('float32'), nb_classes)\n",
    "y_test_seq = np_utils.to_categorical((y_test).astype('float32'), nb_classes)\n",
    "print('X_train shape:', X_train_seq.shape) \n",
    "print('X_test shape:', X_test_seq.shape) \n",
    "print('y_train shape:', y_train_seq.shape)\n",
    "print('y_test shape:', y_test_seq.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82e1563b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 128)         2560000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,691,842\n",
      "Trainable params: 2,691,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Construct a simple LSTM\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(top_words, 128))\n",
    "model1.add(LSTM(128)) \n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(nb_classes))\n",
    "model1.add(Activation('softmax'))\n",
    "model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a40d782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "869/869 [==============================] - 145s 164ms/step - loss: 0.2583 - accuracy: 0.8967\n",
      "Epoch 2/10\n",
      "869/869 [==============================] - 124s 143ms/step - loss: 0.1395 - accuracy: 0.9511\n",
      "Epoch 3/10\n",
      "869/869 [==============================] - 120s 138ms/step - loss: 0.0999 - accuracy: 0.9663\n",
      "Epoch 4/10\n",
      "869/869 [==============================] - 120s 138ms/step - loss: 0.0765 - accuracy: 0.9754\n",
      "Epoch 5/10\n",
      "869/869 [==============================] - 150s 172ms/step - loss: 0.0598 - accuracy: 0.9811\n",
      "Epoch 6/10\n",
      "869/869 [==============================] - 181s 208ms/step - loss: 0.0571 - accuracy: 0.9810\n",
      "Epoch 7/10\n",
      "869/869 [==============================] - 178s 205ms/step - loss: 0.0401 - accuracy: 0.9872\n",
      "Epoch 8/10\n",
      "869/869 [==============================] - 172s 198ms/step - loss: 0.0335 - accuracy: 0.9899\n",
      "Epoch 9/10\n",
      "869/869 [==============================] - 165s 190ms/step - loss: 0.0248 - accuracy: 0.9931\n",
      "Epoch 10/10\n",
      "869/869 [==============================] - 147s 170ms/step - loss: 0.0237 - accuracy: 0.9930\n",
      "97/97 [==============================] - 5s 40ms/step - loss: 0.2866 - accuracy: 0.9353\n",
      "Test loss : 0.2866\n",
      "Test accuracy : 0.9353\n"
     ]
    }
   ],
   "source": [
    "# Compile LSTM\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.fit(X_train_seq, y_train_seq, epochs=nb_epoch,batch_size=batch_size,verbose=1)\n",
    "\n",
    "# Model evluation\n",
    "score = model1.evaluate(X_test_seq, y_test_seq, batch_size=batch_size)\n",
    "print('Test loss : {:.4f}'.format(score[0]))\n",
    "print('Test accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e72c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f57d256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
